{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcGJ6EJX7K79",
        "outputId": "34c17f04-a4d1-4e2f-9646-dcf1a2d006d4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBbu883F8LC9",
        "outputId": "6e96c8cc-7171-4748-f45a-1a1c2b883f23"
      },
      "source": [
        "!pip install -qqq ftfy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████                           | 10 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 51 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 61 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 64 kB 1.6 MB/s \n",
            "\u001b[?25h  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDUl6i7JaSFK",
        "outputId": "a9a6690f-75c9-46d0-885b-6df84fc58db9"
      },
      "source": [
        "!pip install -qqq json_file"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 21.1 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30 kB 33.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 35.7 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 61 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 102 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 122 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 133 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 153 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 163 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 174 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 184 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 194 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 204 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 215 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 225 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 235 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 245 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 256 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 266 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 276 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 286 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 296 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 307 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 317 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 327 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 337 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 348 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 358 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 368 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 378 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 389 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 399 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 409 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 419 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 430 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 440 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 450 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 460 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 471 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 481 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 491 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 501 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 512 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 522 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 532 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 542 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 552 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 563 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 573 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 583 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 593 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 604 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 614 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 624 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 634 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 636 kB 15.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxQxjk46QBLn",
        "outputId": "721f1f64-2dd6-4a81-8343-5ceb2b39d473"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=64518b9d7571fd8219922cbd13526b651533def8a681a4fa6b9a2eb09df9fba2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qo_4113h/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PlVghMHQFau",
        "outputId": "65bff29b-77e2-496f-9272-3822ac2bbcb1"
      },
      "source": [
        "!pip install -U SpaCy==2.2.0"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpaCy==2.2.0\n",
            "  Downloading spacy-2.2.0-cp37-cp37m-manylinux1_x86_64.whl (10.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.2 MB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from SpaCy==2.2.0) (1.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from SpaCy==2.2.0) (2.23.0)\n",
            "Collecting thinc<7.2.0,>=7.1.1\n",
            "  Downloading thinc-7.1.1-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 40.4 MB/s \n",
            "\u001b[?25hCollecting plac<1.0.0,>=0.9.6\n",
            "  Downloading plac-0.9.6-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from SpaCy==2.2.0) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from SpaCy==2.2.0) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from SpaCy==2.2.0) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from SpaCy==2.2.0) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from SpaCy==2.2.0) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from SpaCy==2.2.0) (1.0.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->SpaCy==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->SpaCy==2.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->SpaCy==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->SpaCy==2.2.0) (2021.10.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.2.0,>=7.1.1->SpaCy==2.2.0) (4.62.3)\n",
            "Installing collected packages: plac, thinc, SpaCy\n",
            "  Attempting uninstall: plac\n",
            "    Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: SpaCy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 2.2.5 requires spacy>=2.2.2, but you have spacy 2.2.0 which is incompatible.\n",
            "en-core-web-lg 2.2.5 requires spacy>=2.2.2, but you have spacy 2.2.0 which is incompatible.\u001b[0m\n",
            "Successfully installed SpaCy-2.2.0 plac-0.9.6 thinc-7.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1etuibcz7QK9",
        "outputId": "8995fe94-67c9-437e-c314-0eadd908fdaf"
      },
      "source": [
        "## Import required libraries\n",
        "\n",
        "## warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "## for data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "## for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "## Bag of Words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "## TF-IDF \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "## Train-Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## for processing\n",
        "import nltk\n",
        "import re\n",
        "import ftfy\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "## Feature selection\n",
        "from sklearn import feature_selection\n",
        "\n",
        "## Support vector machine\n",
        "from sklearn.pipeline import Pipeline\n",
        "import sklearn.metrics as skm\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "## for saving and loading model\n",
        "import pickle\n",
        "\n",
        "## for word embedding with Spacy\n",
        "import spacy\n",
        "import en_core_web_lg\n",
        "\n",
        "# ## for word embedding\n",
        "# import gensim\n",
        "# import gensim.downloader as gensim_api\n",
        "# from gensim.models import Word2Vec\n",
        "# from gensim.models import KeyedVectors\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# ## for deep learning\n",
        "# from keras.models import load_model\n",
        "# from keras.models import Model, Sequential\n",
        "# from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "# from keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D\n",
        "# from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
        "# from tensorflow.keras import backend as K\n",
        "# from keras.models import model_from_json\n",
        "# from keras.layers import Lambda\n",
        "# import tensorflow as tf\n",
        "# import json\n",
        "# import json_file"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et0P51Hh7nHH"
      },
      "source": [
        "# Expand Contraction\n",
        "cList = {\n",
        "  \"ain't\": \"am not\",\n",
        "  \"aren't\": \"are not\",\n",
        "  \"can't\": \"cannot\",\n",
        "  \"can't've\": \"cannot have\",\n",
        "  \"'cause\": \"because\",\n",
        "  \"could've\": \"could have\",\n",
        "  \"couldn't\": \"could not\",\n",
        "  \"couldn't've\": \"could not have\",\n",
        "  \"didn't\": \"did not\",\n",
        "  \"doesn't\": \"does not\",\n",
        "  \"don't\": \"do not\",\n",
        "  \"hadn't\": \"had not\",\n",
        "  \"hadn't've\": \"had not have\",\n",
        "  \"hasn't\": \"has not\",\n",
        "  \"haven't\": \"have not\",\n",
        "  \"he'd\": \"he would\",\n",
        "  \"he'd've\": \"he would have\",\n",
        "  \"he'll\": \"he will\",\n",
        "  \"he'll've\": \"he will have\",\n",
        "  \"he's\": \"he is\",\n",
        "  \"how'd\": \"how did\",\n",
        "  \"how'd'y\": \"how do you\",\n",
        "  \"how'll\": \"how will\",\n",
        "  \"how's\": \"how is\",\n",
        "  \"I'd\": \"I would\",\n",
        "  \"I'd've\": \"I would have\",\n",
        "  \"I'll\": \"I will\",\n",
        "  \"I'll've\": \"I will have\",\n",
        "  \"I'm\": \"I am\",\n",
        "  \"I've\": \"I have\",\n",
        "  \"isn't\": \"is not\",\n",
        "  \"it'd\": \"it had\",\n",
        "  \"it'd've\": \"it would have\",\n",
        "  \"it'll\": \"it will\",\n",
        "  \"it'll've\": \"it will have\",\n",
        "  \"it's\": \"it is\",\n",
        "  \"let's\": \"let us\",\n",
        "  \"ma'am\": \"madam\",\n",
        "  \"mayn't\": \"may not\",\n",
        "  \"might've\": \"might have\",\n",
        "  \"mightn't\": \"might not\",\n",
        "  \"mightn't've\": \"might not have\",\n",
        "  \"must've\": \"must have\",\n",
        "  \"mustn't\": \"must not\",\n",
        "  \"mustn't've\": \"must not have\",\n",
        "  \"needn't\": \"need not\",\n",
        "  \"needn't've\": \"need not have\",\n",
        "  \"o'clock\": \"of the clock\",\n",
        "  \"oughtn't\": \"ought not\",\n",
        "  \"oughtn't've\": \"ought not have\",\n",
        "  \"shan't\": \"shall not\",\n",
        "  \"sha'n't\": \"shall not\",\n",
        "  \"shan't've\": \"shall not have\",\n",
        "  \"she'd\": \"she would\",\n",
        "  \"she'd've\": \"she would have\",\n",
        "  \"she'll\": \"she will\",\n",
        "  \"she'll've\": \"she will have\",\n",
        "  \"she's\": \"she is\",\n",
        "  \"should've\": \"should have\",\n",
        "  \"shouldn't\": \"should not\",\n",
        "  \"shouldn't've\": \"should not have\",\n",
        "  \"so've\": \"so have\",\n",
        "  \"so's\": \"so is\",\n",
        "  \"that'd\": \"that would\",\n",
        "  \"that'd've\": \"that would have\",\n",
        "  \"that's\": \"that is\",\n",
        "  \"there'd\": \"there had\",\n",
        "  \"there'd've\": \"there would have\",\n",
        "  \"there's\": \"there is\",\n",
        "  \"they'd\": \"they would\",\n",
        "  \"they'd've\": \"they would have\",\n",
        "  \"they'll\": \"they will\",\n",
        "  \"they'll've\": \"they will have\",\n",
        "  \"they're\": \"they are\",\n",
        "  \"they've\": \"they have\",\n",
        "  \"to've\": \"to have\",\n",
        "  \"wasn't\": \"was not\",\n",
        "  \"we'd\": \"we had\",\n",
        "  \"we'd've\": \"we would have\",\n",
        "  \"we'll\": \"we will\",\n",
        "  \"we'll've\": \"we will have\",\n",
        "  \"we're\": \"we are\",\n",
        "  \"we've\": \"we have\",\n",
        "  \"weren't\": \"were not\",\n",
        "  \"what'll\": \"what will\",\n",
        "  \"what'll've\": \"what will have\",\n",
        "  \"what're\": \"what are\",\n",
        "  \"what's\": \"what is\",\n",
        "  \"what've\": \"what have\",\n",
        "  \"when's\": \"when is\",\n",
        "  \"when've\": \"when have\",\n",
        "  \"where'd\": \"where did\",\n",
        "  \"where's\": \"where is\",\n",
        "  \"where've\": \"where have\",\n",
        "  \"who'll\": \"who will\",\n",
        "  \"who'll've\": \"who will have\",\n",
        "  \"who's\": \"who is\",\n",
        "  \"who've\": \"who have\",\n",
        "  \"why's\": \"why is\",\n",
        "  \"why've\": \"why have\",\n",
        "  \"will've\": \"will have\",\n",
        "  \"won't\": \"will not\",\n",
        "  \"won't've\": \"will not have\",\n",
        "  \"would've\": \"would have\",\n",
        "  \"wouldn't\": \"would not\",\n",
        "  \"wouldn't've\": \"would not have\",\n",
        "  \"y'all\": \"you all\",\n",
        "  \"y'alls\": \"you alls\",\n",
        "  \"y'all'd\": \"you all would\",\n",
        "  \"y'all'd've\": \"you all would have\",\n",
        "  \"y'all're\": \"you all are\",\n",
        "  \"y'all've\": \"you all have\",\n",
        "  \"you'd\": \"you had\",\n",
        "  \"you'd've\": \"you would have\",\n",
        "  \"you'll\": \"you you will\",\n",
        "  \"you'll've\": \"you you will have\",\n",
        "  \"you're\": \"you are\",\n",
        "  \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
        "\n",
        "def expandContractions(text, c_re=c_re):\n",
        "    def replace(match):\n",
        "        return cList[match.group(0)]\n",
        "    return c_re.sub(replace, text)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYC9ogX68On6"
      },
      "source": [
        "## Function to perform stepwise cleaning process\n",
        "def tweets_cleaner(tweet):\n",
        "  cleaned_tweets = []\n",
        "  tweet = tweet.lower() #lowercase\n",
        "    \n",
        "  # if url links then don't append to avoid news articles\n",
        "  # also check tweet length, save those > 5 \n",
        "  if re.match(\"(\\w+:\\/\\/\\S+)\", tweet) == None and len(tweet) > 5:\n",
        "    \n",
        "    #remove hashtag, @mention, emoji and image URLs\n",
        "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(\\#[A-Za-z0-9]+)|(<Emoji:.*>)|(pic\\.twitter\\.com\\/.*)\", \" \", tweet).split())\n",
        "\n",
        "    #fix weirdly encoded texts\n",
        "    tweet = ftfy.fix_text(tweet)\n",
        "\n",
        "    #expand contraction\n",
        "    tweet = expandContractions(tweet)\n",
        "\n",
        "\n",
        "    #remove punctuation\n",
        "    tweet = ' '.join(re.sub(\"([^0-9A-Za-z \\t])\", \" \", tweet).split())\n",
        "\n",
        "    #stop words and lemmatization\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = nltk.word_tokenize(tweet)\n",
        "\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    filtered_sentence = [lemmatizer.lemmatize(word) for word in word_tokens if not word in stop_words]\n",
        "    # back to string from list\n",
        "    tweet = ' '.join(filtered_sentence) # join words with a space in between them\n",
        "\n",
        "    cleaned_tweets.append(tweet)\n",
        "\n",
        "  return cleaned_tweets"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKDR_mUeP7GZ"
      },
      "source": [
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYfjSSKBM2k9",
        "outputId": "2e7b6b16-9395-4d1b-b45d-9ac1dc705b20"
      },
      "source": [
        "## Load the model\n",
        "SVM = \"/content/drive/MyDrive/NLP/Depression_Detection/modeling/model_svm.pkl\" \n",
        "with open(SVM, 'rb') as file:  \n",
        "    clf = pickle.load(file)\n",
        "\n",
        "clf"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
              "    verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxADjK0S8Syn"
      },
      "source": [
        "test_tweet = \"I hate my life\""
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeSjo7QZ8qtN"
      },
      "source": [
        "corpus = tweets_cleaner(test_tweet)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9z5L1pJ8zji",
        "outputId": "040e182f-204d-4254-90b5-5f3e06d56a84"
      },
      "source": [
        "corpus"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hate life']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q4mCzlwPuG2"
      },
      "source": [
        "## word-embedding\n",
        "test = pd.np.array([pd.np.array([token.vector for token in nlp(s)]).mean(axis=0) * pd.np.ones((300)) \\\n",
        "                           for s in corpus])"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3TkyvFqRJ3k"
      },
      "source": [
        "labels_pred = clf.predict(test)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEt81GoURXI7",
        "outputId": "78ce71c9-b0a5-4274-9f9d-6b8f0ae36f04"
      },
      "source": [
        "labels_pred[0]"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV2IT5YU9zjo",
        "outputId": "ca4df9a9-e231-4fd8-ba7a-eeeba7a71d36"
      },
      "source": [
        "# loaded_model = model_from_json(open(\"/content/drive/MyDrive/NLP/Depression_Detection/modeling/model.json\", \"r\").read(),\n",
        "#                               custom_objects={'tf': tf}) \n",
        "# # load weights into new model\n",
        "# loaded_model.load_weights(\"/content/drive/MyDrive/NLP/Depression_Detection/modeling/model.h5\")\n",
        "# print(\"Loaded model from disk\")"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model from disk\n"
          ]
        }
      ]
    }
  ]
}